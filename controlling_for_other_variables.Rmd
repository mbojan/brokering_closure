---
title: "Controlling for other variables"
author: "Michał Bojanowski"
date: "July 22, 2016"
output: 
  html_document:
    toc: yes
    number_sections: yes
---


---

```{r setup, include=FALSE}
library(dplyr)
library(broom)
library(tidyr)
library(knitr)
library(lavaan)
library(semPlot)

knitr::opts_chunk$set(
  echo = TRUE
  )

set.seed(0)
```



- What does it mean to "control for other variables" in multivariate analysis?
- Co oznacza stwierdzenie, że "efekt zmiennej $X$ jest, *ceteris paribus* równy $\beta_x$"?

Poniżej ilustracje, dla zmiennych ciągłych i dyskretnych.










# Reality

We might think of variables connected causally in the following way:

```{r data}
mod <- "
  y ~ 1*x1 + 2*x2 + 3*x3 + 4*x4
  x3 ~ 1.2 * x2
  x1 ~~ 0*x4
  x2 ~~ 0*x4
  x1 ~~ 0.2*x2
"
d <- simulateData(mod, sample.nobs = 1000)
fit <- sem(mod, data=d, fixed.x=FALSE)
semPaths(fit, "path", fixedStyle = 1)
```

- All variables cause $y$
- Variable x2 also causes x3
- Variables x1 and x2 might be correlated due to common causes that do not interest us
- Variable x4 is *not* correlated with x1, x2, and x3.

In data this might look as a dataset with `r ncol(d)` columns for variables: 
`r paste(names(d), collapse=", ")` measured on `r nrow(d)` subjects.
Few first rows of such dataset might look like this:

```{r}
d %>% head() %>% kable()
```

All variables are centered.

```{r}
d %>%
  summarise_each(
    funs(mean, sd)
  ) %>%
  gather(what, value) %>%
  separate(what, c("Variable", "f")) %>%
  spread(f, value)
```



Variables are correlated, which can be assessed with a correlation matrix:

```{r}
d %>% cor() %>% kable()
```

... or a matrix of scatterplots:

```{r}
pairs(d)
```



## Efekt

Chcemy poznać *efekt* zmiennej $X_1$ na zmienną $Y$.

> Przez **efekt** zmiennej $X$ na zmienną $Y$ rozumiemy oczekiwaną różnicę w wartościach $Y$ odpowiadającą różnicy w $X$ równej 1.

Inne sformułowania znaczące przeważnie to [prawie] samo:

1. "Oczekiwana zmiana wartości $Y$ gdy wartość $X$ wzrośnie o 1". 
    Sugeruje również, co nie zawsze jest zgodne z rzeczywistością, że zmiana $Y$ jest *wywołana* zmianą $X$, która z kolei jest efektem jakiejś interwencji (np. eksperymentalnej). 
    
    
# Analizy


Pełny model:

```{r}
summary(m0 <- lm( y ~ x1 + x2 + x3 + x4, data=d) )
```

Efekt cząstkowy `x3` jako regresja reszt na reszty:

```{r}
m1 <- lm(y ~ x2 + x1 + x4, data=d)
m2 <- lm(x3 ~ x2 + x1 + x4, data=d)
m3 <- lm( resid(m1) ~ resid(m2) )

summary(m1)
summary(m2)
```


